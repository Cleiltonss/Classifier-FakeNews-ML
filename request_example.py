import requests
import json
import time
from bs4 import BeautifulSoup
import os

URL = "https://api.gdeltproject.org/api/v2/doc/doc?query=Brazil&mode=artlist&format=json&lang=Portuguese"
CACHE_FILE = 'news_cache.json'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
MAX_ATTEMPTS = 10
TIMEOUT = 10
MAX_RECORDS = 250  # N√∫mero m√°ximo de artigos por requisi√ß√£o


# Requisi√ß√£o √† API com pagina√ß√£o
def request_API(url, max_attempts=MAX_ATTEMPTS):
    all_articles = []
    seen_urls = set()  # Armazena URLs j√° coletadas
    offset = 0

    while True:
        paginated_url = f"{url}&maxRecords={MAX_RECORDS}&offset={offset}"
        attempt = 0

        while attempt < max_attempts:
            try:
                response = requests.get(paginated_url, headers=HEADERS, timeout=TIMEOUT)
                print(f'üîÑ Tentativa {attempt + 1} - Status: {response.status_code} (Offset: {offset})')

                if response.status_code == 200:
                    data = response.json()
                    articles = data.get('articles', [])

                    if not articles:
                        print("üö´ Nenhuma not√≠cia adicional encontrada.")
                        return {'articles': all_articles}

                    # Filtra artigos repetidos
                    new_articles = [a for a in articles if a['url'] not in seen_urls]
                    for article in new_articles:
                        seen_urls.add(article['url'])  # Adiciona URL ao conjunto
                        all_articles.append(article)

                    print(f"üìÑ {len(new_articles)} artigos √∫nicos coletados no offset {offset}.")
                    offset += MAX_RECORDS

                    # Se n√£o h√° novos artigos, encerra
                    if len(new_articles) == 0:
                        print("üö´ Todos os artigos j√° foram coletados. Encerrando...")
                        return {'articles': all_articles}

                    break
                elif response.status_code == 429:
                    print('‚ö†Ô∏è Muitas requisi√ß√µes (429). Aguardando 10 segundos...')
                    time.sleep(10)
                else:
                    print(f'‚ùå Erro na requisi√ß√£o: {response.status_code}')
                    break
            except requests.exceptions.RequestException as e:
                print(f'Erro de conex√£o: {e}')
            attempt += 1

        if attempt == max_attempts:
            print("üö´ Limite de tentativas atingido para o offset.")
            break

    return {'articles': all_articles}



# Fun√ß√£o: Extrai o texto completo do artigo usando BeautifulSoup
def extract_article_content(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            paragraphs = soup.find_all('p')
            return ' '.join(p.get_text() for p in paragraphs) or "üî∏ Conte√∫do n√£o encontrado."
        return "‚ùå Falha ao carregar o artigo."
    except requests.exceptions.RequestException as e:
        return f"Erro ao acessar o artigo: {e}"


# Salva no cache (JSON)
def save_to_cache(data, file_path):
    with open(file_path, "w") as f:
        json.dump(data, f, indent=4)
    print("‚úÖ Cache salvo com sucesso.")


# Carrega do cache (se existir)
def load_from_cache(file_path):
    if os.path.exists(file_path):
        print("‚ôªÔ∏è Reutilizando dados do cache.")
        with open(file_path, "r") as f:
            return json.load(f)
    return None


# Fun√ß√£o principal
def main():
    articles_data = request_API(URL)

    if not articles_data or 'articles' not in articles_data:
        print("üö´ Nenhum dado dispon√≠vel.")
        return
    
    # Itera pelos artigos para extrair o conte√∫do completo
    for article in articles_data.get('articles', []):
        print(f"\nüîπ {article['title']}\nüîó {article['url']}")
        article['full_text'] = extract_article_content(article['url'])
        print(f"üìù {article['full_text'][:300]}...\n")

    # Salva em cache
    save_to_cache(articles_data, CACHE_FILE)


# Executa o script
if __name__ == "__main__":
    main()
